# -*- coding: utf-8 -*-
"""HaikuWithTransformers.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1W_7pzeeOJKX7zpoVEpITODCej8CpWsnF

# Submitted By:
### Asad Tariq 19I-0659
### Abdullah Abbasi 19I-2179
### Saim Aslam 19I-0461
"""

import datetime
import time
import pandas as pd
import tensorflow as tf
import json
from pathlib import Path
import re
import random
import inflect
import numpy as np
import os
import seaborn as sns
import matplotlib.pyplot as plt

!pip install transformers

from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config, AdamW, get_linear_schedule_with_warmup

import torch
torch.manual_seed(64)
from torch.utils.data import Dataset, random_split, DataLoader, RandomSampler, SequentialSampler
!pip show torch

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
device

from google.colab import drive
drive.mount('/content/gdrive')

nRowsRead = None # specify 'None' if want to read whole file
#df = pd.read_csv('INPUT/all_haiku.csv', delimiter=',', nrows = nRowsRead)
df=pd.read_csv('/content/gdrive/MyDrive/Work/all_haiku.csv')

df.dataframeName = 'all_haiku.csv'
nRow, nCol = df.shape
print(f'There are {nRow} rows and {nCol} columns')

df=df.drop('Unnamed: 0',axis=1)
df=df.drop('source',axis=1)
df=df.drop('hash',axis=1)

df["2"] = df["2"].astype(str)
df["2"] = [x.replace('-','') for x in df["2"]]
df = df.replace('[^\w\s]', '')
df = df.replace('-','',regex=True)
df.rename(columns = {'0':0,'1':1,'2':2}, inplace = True)

df.head(20)

input_list=[]
#for i in range(144123):
for i in range(10000):
    input_list.append(df[0][i]+'\n'+df[1][i]+'\n'+df[2][i]+'.')

print(input_list[0])

class HaikuDataset(Dataset):
  def __init__(self, haikus, tokenizer, max_length=25, gpt2_type="gpt2"):
    self.tokenizer = tokenizer
    self.input_ids = []
    self.attn_masks = []

    for haiku in haikus:

      encodings_dict = tokenizer("<|startoftext|>"+haiku+"<|endoftext|>",
                                 truncation=True,
                                 max_length=max_length,
                                 padding="max_length")
      
      self.input_ids.append(torch.tensor(encodings_dict["input_ids"]))
      self.attn_masks.append(torch.tensor(encodings_dict["attention_mask"]))

  def __len__(self):
    return len(self.input_ids)

  def __getitem__(self, idx):
    return self.input_ids[idx], self.attn_masks[idx]

# Loading GPT2 Tokenizer
tokenizer = GPT2Tokenizer.from_pretrained('gpt2', 
                                          bos_token='<|startoftext|>', 
                                          eos_token='<|endoftext|>', 
                                          pad_token='<|pad|>')

print(tokenizer.encode("<|startoftext|> Hello World <|endoftext|>", padding="max_length", max_length=10))
print(len(tokenizer))

# Finding length of maximum token in dataset
max_length = max([len(tokenizer.encode(haiku)) for haiku in input_list])
print(max_length)
max_length = 25

x = [len(tokenizer.encode(haiku)) for haiku in input_list if len(tokenizer.encode(haiku)) < 100]

y = [len(tokenizer.encode(haiku)) - len(haiku.split()) for haiku in input_list]

print(sum(y)/len(y))

print(max(x), len(x))
plt.hist(x, bins = 5)
plt.show

batch_size = 10
max_length = 25

dataset = HaikuDataset(input_list, tokenizer, max_length=max_length)

# Split data into train and validation sets
train_size = int(0.9*len(dataset))
val_size = len(dataset) - train_size

train_dataset, val_dataset = random_split(dataset, [train_size, val_size])
print("Number of samples for training =", train_size)
print("Number of samples for validation =", val_size)

train_dataset[0]

train_dataloader = DataLoader(train_dataset,
                              sampler=RandomSampler(train_dataset),
                              batch_size=batch_size)

val_dataloader = DataLoader(val_dataset,
                            sampler=SequentialSampler(val_dataset),
                            batch_size=batch_size)

# Load model configuration
config = GPT2Config.from_pretrained("gpt2")

# Create model instance and set embedding length
model = GPT2LMHeadModel.from_pretrained("gpt2", config=config)
model.resize_token_embeddings(len(tokenizer))

# Running the model on GPU
model = model.to(device)

# Setting seeds to enable reproducible runs
seed_val = 42

random.seed(seed_val)
np.random.seed(seed_val)
torch.manual_seed(seed_val)
torch.cuda.manual_seed_all(seed_val)

epochs = 6
warmup_steps = 1e2
sample_every = 1000

print(len(train_dataloader))
print(len(train_dataset))

optimizer = AdamW(model.parameters(), lr=5e-4, eps=1e-8)

# Toatl training steps is the number of data points times the number of epochs
total_training_steps = len(train_dataloader)*epochs

# Setting a variable learning rate using scheduler
scheduler = get_linear_schedule_with_warmup(optimizer,
                                            num_warmup_steps=warmup_steps,
                                            num_training_steps=total_training_steps)

def format_time(elapsed):
  return str(datetime.timedelta(seconds=int(round(elapsed))))

total_t0 = time.time()

training_stats = []

model = model.to(device)

for epoch_i in range(epochs):
  print(f'Beginning epoch {epoch_i+1} of {epochs}')

  t0 = time.time()
  total_train_loss = 0
  model.train()

  # Labels are shifted by 1 timestep
  for step, batch in enumerate(train_dataloader):
    b_input_ids = batch[0].to(device)
    b_labels = batch[0].to(device)
    b_masks = batch[1].to(device)

    model.zero_grad()

    outputs = model(b_input_ids,
                    labels=b_labels,
                    attention_mask=b_masks)
    
    loss = outputs[0]

    batch_loss = loss.item()
    total_train_loss += batch_loss

    # Sampling every x steps
    if step != 0 and step % sample_every == 0:

      elapsed = format_time(time.time()-t0)
      print(f'Batch {step} of {len(train_dataloader)}. Loss: {batch_loss}. Time: {elapsed}')

      model.eval()

      sample_outputs = model.generate(
                                    bos_token_id=random.randint(1,30000),
                                    do_sample=True,   
                                    top_k=50, 
                                    max_length = 25,
                                    top_p=0.95, 
                                    num_return_sequences=1
                                )
      for i, sample_output in enumerate(sample_outputs):
        print(f'Example ouput: {tokenizer.decode(sample_output, skip_special_tokens=True)}')
      print()

      model.train()

    loss.backward()
    optimizer.step()
    scheduler.step()

  avg_train_loss = total_train_loss / len(train_dataloader)
  training_time = format_time(time.time()-t0)
  print(f'Average Training Loss: {avg_train_loss}. Epoch time: {training_time}')
  print()

  t0 = time.time()
  model.eval()

  total_eval_loss = 0
  nb_eval_steps = 0

  for batch in val_dataloader:
    b_input_ids = batch[0].to(device)
    b_labels = batch[0].to(device)
    b_masks = batch[1].to(device)
        
    with torch.no_grad():        

        outputs  = model(b_input_ids,  
                         attention_mask = b_masks,
                         labels=b_labels)
          
        loss = outputs[0]  
            
    batch_loss = loss.item()
    total_eval_loss += batch_loss   

  avg_val_loss = total_eval_loss / len(val_dataloader)  
  val_time = format_time(time.time() - t0)    
  print(f'Validation loss: {avg_val_loss}. Validation Time: {val_time}')
  print()

  # Record all statistics from this epoch.
  training_stats.append(
      {
          'epoch': epoch_i + 1,
          'Training Loss': avg_train_loss,
          'Valid. Loss': avg_val_loss,
          'Training Time': training_time,
          'Validation Time': val_time
      }
  )
  print("------------------------------")

print(f'Total training took {format_time(time.time()-total_t0)}')



model.eval()

prompt = "<|startoftext|>"

generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)
generated = generated.to(device)

sample_outputs = model.generate(
                                generated, 
                                do_sample=True,   
                                top_k=20, 
                                max_length = 40,
                                top_p=0.8, 
                                num_return_sequences=10
                                )

for i, sample_output in enumerate(sample_outputs):
  print("{}: {}\n\n".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))

model.eval()

prompt = "<|startoftext|>"

generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)
generated = generated.to(device)

sample_outputs = model.generate(
                                generated, 
                                do_sample=True,   
                                top_k=30, 
                                max_length = 25,
                                top_p=0.7, 
                                num_return_sequences=15
                                )

for i, sample_output in enumerate(sample_outputs):
  print("{}: {}\n\n".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))

model.eval()

prompt = "<|startoftext|>"

generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)
generated = generated.to(device)

sample_outputs = model.generate(
                                generated, 
                                do_sample=True,   
                                top_k=20, 
                                max_length = 15,
                                top_p=0.5, 
                                num_return_sequences=5
                                )

for i, sample_output in enumerate(sample_outputs):
  print("{}: {}\n\n".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))

output_dir = "/content/drive/My Drive/Work/GPTmodels/GPT2Tuned"

# Save generated poems
# sample_outputs = model.generate(
#                                 generated, 
#                                 do_sample=True,   
#                                 top_k=50, 
#                                 max_length = 300,
#                                 top_p=0.95, 
#                                 num_return_sequences=25
#                                 )

# with open(os.path.join(output_dir, 'generated_poems.txt'), "w") as outfile:
#   for i, sample_output in enumerate(sample_outputs):
#     outfile.write(tokenizer.decode(sample_output, skip_special_tokens=True)+"\n\n")

# Save a trained model, configuration and tokenizer using `save_pretrained()`.
# They can then be reloaded using `from_pretrained()`
model_to_save = model.module if hasattr(model, 'module') else model
model_to_save.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)

# Good practice: save your training arguments together with the trained model
# torch.save(training_stats, os.path.join(output_dir, 'training_args.bin'))

# Loading saved model
model_dir = "/content/drive/My Drive/Work/GPTmodels/GPT2Tuned"

model = GPT2LMHeadModel.from_pretrained(model_dir)
tokenizer = GPT2Tokenizer.from_pretrained(model_dir)

# model = GPT2LMHeadModel.from_pretrained("prajwalcr/poetry-joy_gpt2")
# tokenizer = GPT2Tokenizer.from_pretrained("prajwalcr/poetry-joy_gpt2")
model.to(device)